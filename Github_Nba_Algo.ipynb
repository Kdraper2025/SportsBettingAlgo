{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce5cb61",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba969429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from retrying import retry\n",
    "from scipy.stats import norm, zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as st\n",
    "from statsmodels.tools import add_constant as add_constant\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79b19d",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3338d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_injury_data(url):\n",
    "    print('here')\n",
    "    response = requests.get(url)\n",
    "    soup = soup_1(response.content, 'html.parser')\n",
    "    # Locate the table containing the injury data\n",
    "    injury_table = soup.find('table', {'id': 'team_injuries'})\n",
    "    if injury_table:\n",
    "        # Read the table using Pandas\n",
    "        injuries_df = pd.read_html(str(injury_table))[0]\n",
    "        \n",
    "        print(injuries_df)\n",
    "        # Clean and process the data if needed\n",
    "        return injuries_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_vegas_lines_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = soup_1(response.content, 'html.parser')\n",
    "    # Locate the table containing the Vegas Lines data\n",
    "    vegas_lines_table = soup.find('table', {'id': 'vegas_lines'})\n",
    "    if vegas_lines_table:\n",
    "        # Read the table using Pandas\n",
    "        vegas_lines_df = pd.read_html(str(vegas_lines_table))[0]\n",
    "\n",
    "        # Clean and process the data if needed\n",
    "        return vegas_lines_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "@retry(stop_max_attempt_number=4, wait_fixed=2000)\n",
    "def team_has_data(team_abbreviation, season):\n",
    "    print(\"TEAM:\", team_abbreviation)\n",
    "    time.sleep(random.randint(3,4))\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/teams/{team_abbreviation}/{season}/gamelog-advanced'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code)\n",
    "        return False  # URL doesn't exist or couldn't be accessed\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'tgl_advanced'})\n",
    "    return table is not None\n",
    "\n",
    "\n",
    "def get_latest_week( specific_season, data):\n",
    "    nba_df = pd.DataFrame()\n",
    "\n",
    "    for season in range(specific_season, specific_season+1):\n",
    "        for a in abbreviations:\n",
    "            if not team_has_data(a, season):\n",
    "                print(team_has_data(a,season))\n",
    "                continue\n",
    "            else:\n",
    "                print('here')\n",
    "                url = f'https://www.basketball-reference.com/teams/{a}/{season}/gamelog-advanced'\n",
    "                injury_url = f'https://www.basketball-reference.com/teams/{a}/{season}.html#all_injuries'\n",
    "                salary_url = f'https://www.basketball-reference.com/teams/{a}/{season}.html'\n",
    "\n",
    "\n",
    "                time.sleep(random.randint(5,6))\n",
    "                team_df = pd.read_html(url, header=1, attrs={'id': 'tgl_advanced'} )[0]\n",
    "                #             def_df = pd.read_html(url, header=1, attrs={'id': 'gamelog_opp' + f'{season}'} )[0]\n",
    "\n",
    "    #             def_df.columns =def_df.columns + '_2'\n",
    "    #             team_df = pd.concat([team_df, def_df], axis=1)\n",
    "                team_df.insert(loc=0, column='Season', value=season +1)\n",
    "                team_df.insert(loc=2, column='Team', value=a.upper())\n",
    "            \n",
    "                team_df['Rk'] = pd.to_numeric(team_df['Rk'], errors='coerce')\n",
    "                latest_game_number = int(team_df['Rk'].max())\n",
    "                \n",
    "                data['Rk'] = pd.to_numeric(data['Rk'], errors='coerce')\n",
    "                filtered_data = data.loc[(data['Season'] == specific_season+1) & (data['Team'] == a)]\n",
    "                \n",
    "                \n",
    "                current_game_number = int(filtered_data['Rk'].max())\n",
    "                \n",
    "                latest_games= pd.DataFrame()\n",
    "                print(current_game_number,'Current Game', latest_game_number, 'Latest_Game')\n",
    "                if current_game_number == latest_game_number:\n",
    "                    print('Games already updated : Continuing')\n",
    "                    continue\n",
    "                print(\"Still going?\")\n",
    "                for game in range(current_game_number+1, latest_game_number+1):\n",
    "                    latest_games = pd.concat([latest_games, team_df[team_df['Rk'] == game]])\n",
    "\n",
    "                team_df = latest_games\n",
    "                \n",
    "                \n",
    "                \n",
    "                try:\n",
    "                # Fetch the salary page\n",
    "                    time.sleep(random.randint(3,5))\n",
    "\n",
    "    #                 response = requests.get(salary_url)\n",
    "    #                 response.raise_for_status()  # Check for request errors\n",
    "\n",
    "                    # Parse the tables in the salary page\n",
    "                    all_tables = pd.read_html(salary_url, header=0)\n",
    "\n",
    "                    # Filter the salary table by its id 'salaries2'\n",
    "                    experience =all_tables[0]['Exp']\n",
    "                    experience = experience.replace({'R': 1, 'TBD': 1, '': 0}, regex=True)\n",
    "                    experience = experience.fillna(0).astype(int)\n",
    "                    total_experience = experience.sum()\n",
    "                    team_df['Experience'] = total_experience\n",
    "    #                 print(team_df['Experience'])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching experience table for {a} in {season}: {e}\")\n",
    "                    team_df['Experience'] = np.nan\n",
    "                    \n",
    "                    \n",
    "                try:\n",
    "                    response = requests.get(salary_url)\n",
    "                    page = re.sub(\"<!--|-->\",\"\",response.text)\n",
    "\n",
    "                    soup = BeautifulSoup(page, 'html.parser')\n",
    "    #                 salary = pd.read_html(salary_url, header=0 )\n",
    "    #                 print(soup.prettify())\n",
    "\n",
    "    #                 print(salary)\n",
    "    #                 salary_table = soup2.find('table', {'id': 'salaries2'})\n",
    "                    # Fetch the salary page\n",
    "                    time.sleep(random.randint(3,5))\n",
    "\n",
    "\n",
    "\n",
    "                    # Parse the tables in the salary page\n",
    "                    all_tables = pd.read_html(page, header=0)\n",
    "\n",
    "                    # Filter the salary table by its id 'salaries2'\n",
    "                    salary = all_tables[-1]['Salary'].replace('[\\$,]', '', regex=True).fillna(0).astype(int)\n",
    "                    salary = salary.sum()\n",
    "                    team_df['Total_Salary'] = salary\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching salary table for {a} in {season}: {e}\")\n",
    "                    team_df['Total_Salary'] = np.nan\n",
    "                \n",
    "                nba_df = pd.concat([nba_df, team_df], ignore_index=True)\n",
    "                nba_df['Experience'] = nba_df.groupby(['Season', 'Team_Code'])['Experience'].apply(lambda x: x.fillna(method='ffill'))\n",
    "                nba_df['Total_Salary'] = nba_df.groupby(['Season', 'Team_Code'])['Total_Salary'].apply(lambda x: x.fillna(method='ffill'))\n",
    "\n",
    "    \n",
    "    data = pd.concat([data, nba_df], ignore_index=True)\n",
    "    data.to_csv('nba_data2', index=False)\n",
    "                \n",
    "\n",
    "def convert_date(date_str):\n",
    "    Month, Day = date_str.split()\n",
    "    month_number = month_mapping.get(Month, None)\n",
    "    \n",
    "    date = f'{month_number}-{int(Day):02d}'\n",
    "    print(date)\n",
    "    return date\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b7dfc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_averages_OPP(group, cols, new_cols):\n",
    "        group = group.sort_values(\"Date\")\n",
    "        rolling_stats = group[cols].rolling(15, closed='left').mean()\n",
    "        group[new_cols] = rolling_stats\n",
    "        season = int(group['Season'][:1])\n",
    "        code = float(group['Opp_Code'][:1])\n",
    "        previous=nba_df[(nba_df['Season'] == (season-1)) & (nba_df['Opp_Code'] == code)]\n",
    "        previous = previous.sort_values(\"Date\")\n",
    "\n",
    "        try:\n",
    "            for a,b in zip(cols,new_cols):\n",
    "                previous_rolling= previous[a].rolling(25,closed='right').mean()\n",
    "    \n",
    "                group[b].fillna(previous_rolling.iloc[-1], inplace=True)\n",
    "        except:\n",
    "            print(\"Didn't make it. \")\n",
    "        \n",
    "        return group\n",
    "            \n",
    "\n",
    "def rolling_averages_TM(group, cols, new_cols):\n",
    "        group = group.sort_values(\"Date\")\n",
    "        rolling_stats = group[cols].rolling(15, closed='left').mean()\n",
    "        group[new_cols] = rolling_stats\n",
    "        season = int(group['Season'][:1])\n",
    "        code = float(group['Team_Code'][:1])\n",
    "        previous=nba_df[(nba_df['Season'] == (season-1)) & (nba_df['Team_Code'] == code)]\n",
    "        previous = previous.sort_values(\"Date\")\n",
    "\n",
    "        try:\n",
    "            for a,b in zip(cols,new_cols):\n",
    "                previous_rolling= previous[a].rolling(25,closed='right').mean()\n",
    "    \n",
    "                group[b].fillna(previous_rolling.iloc[-1], inplace=True)\n",
    "        except:\n",
    "            print(\"Didn't make it. \")\n",
    "\n",
    "        return group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430f20c",
   "metadata": {},
   "source": [
    "# Scrape Nba Data from 1985 to 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3720656",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df = pd.DataFrame()\n",
    "import re\n",
    "\n",
    "for season in range(1985, 2025):\n",
    "    for a in abbreviations:\n",
    "        if not team_has_data(a, season):\n",
    "            print(team_has_data(a,season))\n",
    "            continue\n",
    "        else:\n",
    "            print('here')\n",
    "            url = f'https://www.basketball-reference.com/teams/{a}/{season}/gamelog-advanced'\n",
    "            injury_url = f'https://www.basketball-reference.com/teams/{a}/{season}.html#all_injuries'\n",
    "            salary_url = f'https://www.basketball-reference.com/teams/{a}/{season}.html'\n",
    "            \n",
    "            \n",
    "            time.sleep(random.randint(3,6))\n",
    "            team_df = pd.read_html(url, header=1, attrs={'id': 'tgl_advanced'} )[0]\n",
    "            #             def_df = pd.read_html(url, header=1, attrs={'id': 'gamelog_opp' + f'{season}'} )[0]\n",
    "            \n",
    "#             def_df.columns =def_df.columns + '_2'\n",
    "#             team_df = pd.concat([team_df, def_df], axis=1)\n",
    "            team_df.insert(loc=0, column='Season', value=season +1)\n",
    "            team_df.insert(loc=2, column='Team', value=a.upper())\n",
    "            print(salary_url)\n",
    "            try:\n",
    "                # Fetch the salary page\n",
    "                time.sleep(random.randint(3,5))\n",
    "\n",
    "#                 response = requests.get(salary_url)\n",
    "#                 response.raise_for_status()  # Check for request errors\n",
    "\n",
    "                # Parse the tables in the salary page\n",
    "                all_tables = pd.read_html(salary_url, header=0)\n",
    "                \n",
    "                # Filter the salary table by its id 'salaries2'\n",
    "                experience =all_tables[0]['Exp']\n",
    "                experience = experience.replace({'R': 1, 'TBD': 1, '': 0}, regex=True)\n",
    "                experience = experience.fillna(0).astype(int)\n",
    "                total_experience = experience.sum()\n",
    "                team_df['Experience'] = total_experience\n",
    "#                 print(team_df['Experience'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching experience table for {a} in {season}: {e}\")\n",
    "                team_df['Experience'] = 0\n",
    "                \n",
    "                \n",
    "            try:\n",
    "                response = requests.get(salary_url)\n",
    "                page = re.sub(\"<!--|-->\",\"\",response.text)\n",
    "\n",
    "                soup = BeautifulSoup(page, 'html.parser')\n",
    "#                 salary = pd.read_html(salary_url, header=0 )\n",
    "#                 print(soup.prettify())\n",
    "                \n",
    "#                 print(salary)\n",
    "#                 salary_table = soup2.find('table', {'id': 'salaries2'})\n",
    "                # Fetch the salary page\n",
    "                time.sleep(random.randint(3,5))\n",
    "\n",
    "                \n",
    "\n",
    "                # Parse the tables in the salary page\n",
    "                all_tables = pd.read_html(page, header=0)\n",
    "                \n",
    "                # Filter the salary table by its id 'salaries2'\n",
    "                salary = all_tables[-1]['Salary'].replace('[\\$,]', '', regex=True).fillna(0).astype(int)\n",
    "                salary = salary.sum()\n",
    "                team_df['Total_Salary'] = salary\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching salary table for {a} in {season}: {e}\")\n",
    "                team_df['Total_Salary'] = 0\n",
    "                \n",
    "            \n",
    "                \n",
    "            nba_df = pd.concat([nba_df, team_df], ignore_index=True)\n",
    "            print(url)\n",
    "nba_df.to_csv('nba_data_SalaryEXP1985', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd50ff28",
   "metadata": {},
   "source": [
    "# Begin Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a7899229",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df = pd.read_csv('nba_data_SalaryEXP1985')\n",
    "\n",
    "\n",
    "nba_df['Season'] = nba_df['Season'] -1\n",
    "nba_df.rename(columns= {'eFG%.1': 'OppeFG%', 'TOV%.1': 'OppTOV%', 'FT/FGA.1': 'OppFT/FGA', 'Unnamed: 3': 'Home/Away',}, inplace=True)\n",
    "nba_df.rename(columns={'Unnamed: 3': 'Home/Away'}, inplace =True)\n",
    "threshold = len(nba_df.columns) * 0.75\n",
    "nba_df = nba_df.dropna(thresh=threshold)\n",
    "nba_df.drop(columns=['Rk', 'Unnamed: 18', 'Unnamed: 23'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7f679",
   "metadata": {},
   "source": [
    "# Made Team Codes for each team and Opponents to track stats across games and seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50eee22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_abbreviations = {\n",
    "    'ATL' : 0, 'BOS' : 1, 'BRK' : 2, 'CHO': 3, 'CHI': 4, 'CLE': 5, 'DAL': 6, 'DEN': 7, 'DET':8, 'GSW':9, 'HOU':10, \n",
    "    'IND':11, 'LAC':12, 'LAL':13, 'MEM':14, 'MIA':15, 'MIL':16, 'MIN':17, 'NOP':18, 'NYK':19, 'OKC':20, 'ORL':21, \n",
    "    'PHI':22, 'PHO':23, 'POR':24, 'SAC':25, 'SAS':26, 'TOR':27, 'UTA':28, 'WAS':29, 'VAN': 14,'NJN': 2, 'CHA':3, 'SEA': 20\n",
    "    ,'NOH': 18, 'NOK': 3, 'CHH': 3, 'WSB' :29, 'KCK':25,'SDC':12,'NOJ':28\n",
    "}\n",
    "\n",
    "nba_df['Team_Code'] = nba_df['Team'].map(mapped_abbreviations)\n",
    "nba_df['Opp_Code'] = nba_df['Opp'].map(mapped_abbreviations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d01c79",
   "metadata": {},
   "source": [
    "# Preparing my Dataset for rolling statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3648bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_move = nba_df.pop('Team_Code')\n",
    "nba_df.insert(2, \"Team_Code\", column_move)\n",
    "column_move = nba_df.pop('Opp_Code')\n",
    "nba_df.insert(5, \"Opp_Code\", column_move)\n",
    "\n",
    "cols = ['Tm', 'Opp.1', 'ORtg', 'DRtg', 'Pace', 'FTr', '3PAr', 'TS%', 'TRB%', 'AST%', \n",
    "    'STL%', 'BLK%', 'eFG%', 'TOV%', 'ORB%', 'FT/FGA', 'OppeFG%', 'OppTOV%', \n",
    "    'DRB%', 'OppFT/FGA'\n",
    "]\n",
    "\n",
    "nba_df[cols] = nba_df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "nba_df['eDIFF'] = nba_df['ORtg'] - nba_df['DRtg']\n",
    "nba_df['pDIFF'] = nba_df['Tm'] -nba_df['Opp.1']\n",
    "cols = ['Tm', 'Opp.1', 'ORtg', 'DRtg', 'Pace', 'FTr', '3PAr', 'TS%', 'TRB%', 'AST%', \n",
    "    'STL%', 'BLK%', 'eFG%', 'TOV%', 'ORB%', 'FT/FGA', 'OppeFG%', 'OppTOV%', \n",
    "    'DRB%', 'OppFT/FGA','eDIFF', 'pDIFF'\n",
    "]\n",
    "\n",
    "new_cols = [f'{c}_rolling' for c in cols]\n",
    "new_longer_cols = [f'{c}_Off_rolling' for c in cols]\n",
    "\n",
    "Opp_cols = ['Tm', 'Opp.1', 'ORtg', 'DRtg', 'eFG%', 'TOV%',  'FT/FGA', 'OppeFG%', 'OppTOV%', \n",
    "     'OppFT/FGA', 'eDIFF', 'pDIFF']\n",
    "\n",
    "\n",
    "new_opp_cols = ['OppPoints_Allowed_rolling', 'OppPoints_Scored_rolling', 'OppDRtg_rolling', 'OppORtg_rolling','DefenisveeFG%_rolling'\n",
    "                , 'DefensiveTOV%_rolling', 'OppOppFT/FGA_rolling','OppOppeFG_rolling','OppOppTOV%_rolling', 'Opp2FT/FGA_rolling', 'OppeDIFF_rolling', 'OpppDIFF_rolling']\n",
    "\n",
    "new_opp_longer_cols = [f'{c}_Def_longer' for c in new_opp_cols]\n",
    "nba_df[cols] = nba_df[cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642db5c",
   "metadata": {},
   "source": [
    "# Rolling statistics of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ddcb34b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n"
     ]
    }
   ],
   "source": [
    "OppNba_df = nba_df.groupby([\"Season\",\"Opp_Code\"]).apply(lambda x: rolling_averages_OPP(x, Opp_cols, new_opp_cols))\n",
    "\n",
    "OppNba_df.index = range(OppNba_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "acd386db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n",
      "Didn't make it. \n"
     ]
    }
   ],
   "source": [
    "TmNba_df = nba_df.groupby([\"Season\",\"Team_Code\"]).apply(lambda x: rolling_averages_TM(x, cols, new_cols))\n",
    "TmNba_df.index = range(TmNba_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cad97ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TmNba_df = TmNba_df.dropna(subset=new_cols)\n",
    "OppNba_df = OppNba_df.dropna(subset=new_opp_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415e9ba",
   "metadata": {},
   "source": [
    "# Merge the datasets back together and further cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25bf32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_nba_df = pd.merge(TmNba_df, OppNba_df, on=['Season', 'G', 'Team_Code'])\n",
    "columns_to_drop = [col for col in Merged_nba_df.columns if col.endswith('_y')]\n",
    "Merged_nba_df = Merged_nba_df.drop(columns=columns_to_drop)\n",
    "Merged_nba_df.columns= Merged_nba_df.columns.str.replace('_x', '')\n",
    "Merged_nba_df['Home/Away'].fillna(1, inplace=True)\n",
    "Merged_nba_df['Home/Away'].replace('@', 0, inplace=True)\n",
    "Merged_nba_df['Date'] = pd.to_datetime(Merged_nba_df['Date'])\n",
    "Merged_nba_df['Opp_Code'] = Merged_nba_df['Opp_Code'].astype(int)\n",
    "Merged_nba_df['Unique_ID'] = Merged_nba_df.apply(lambda row: f\"{row['Season']}_{row['G']}_{row['Team_Code']}@{row['Opp_Code']}\" if row['Home/Away'] == 0 else f\"{row['Season']}_{row['G']}_{row['Opp_Code']}@{row['Team_Code']}\", axis=1)\n",
    "Merged_nba_df.sort_values(by='Date', inplace=True)\n",
    "Merged_nba_df['Total_Points'] = Merged_nba_df['Tm'] + Merged_nba_df['Opp.1']\n",
    "Merged_nba_df['OppeDIFF_rolling'] = -Merged_nba_df['OppeDIFF_rolling']\n",
    "Merged_nba_df['OpppDIFF_rolling'] = -Merged_nba_df['OpppDIFF_rolling']\n",
    "Merged_nba_df['G'] = Merged_nba_df['G'].astype(int)\n",
    "duplicates = Merged_nba_df.duplicated(subset=['Unique_ID'], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e211b",
   "metadata": {},
   "source": [
    "# Making Targets for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6498cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_nba_df['W/L'].replace('W', 1, inplace =True)\n",
    "Merged_nba_df['W/L'].replace('L', 0, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b848a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5175e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf475a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "64cad9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_nba_df['Date'] = pd.to_datetime(Merged_nba_df['Date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662908ce",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "644f6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_nba_df['Rest_Days'] = Merged_nba_df.groupby([\"Season\", \"Team_Code\"])['Date'].diff().dt.days\n",
    "Merged_nba_df['Rest_Days'].clip(lower=0, inplace=True)\n",
    "Merged_nba_df['Rest_Days'].fillna(0,inplace=True)\n",
    "Merged_nba_df['Rest_Days'].clip(upper=9, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0943de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_nba_df['Opp_Rest_Days'] = Merged_nba_df.groupby(['Season', 'Opp_Code'])['Date'].diff().dt.days\n",
    "Merged_nba_df['Opp_Rest_Days'].clip(lower=0, inplace=True)\n",
    "Merged_nba_df['Opp_Rest_Days'].fillna(0,inplace=True)\n",
    "Merged_nba_df['Opp_Rest_Days'].clip(upper=9, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7caf4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_nba_df['TmeDIFFProduct'] = Merged_nba_df['eDIFF_rolling'] * Merged_nba_df['Tm_rolling']\n",
    "Merged_nba_df['TmpDIFFProduct'] = Merged_nba_df['pDIFF_rolling'] * Merged_nba_df['Tm_rolling']\n",
    "Merged_nba_df['TmOppAllowedProduct'] = Merged_nba_df['Tm_rolling'] * Merged_nba_df['OppPoints_Allowed_rolling']\n",
    "Merged_nba_df['PaceTmProduct'] = Merged_nba_df['Pace_rolling'] * Merged_nba_df['Tm_rolling']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "34553487",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_nba_df['Expanding_Total_Points'] = Merged_nba_df.groupby(['Season', 'Team_Code'])['Total_Points'].apply(lambda x: x.shift(1).expanding(min_periods=1).mean()).reset_index(level=[0, 1], drop=True)\n",
    "Merged_nba_df['Expanding_Opp_Total_Points'] = Merged_nba_df.groupby(['Season', 'Opp_Code'])['Total_Points'].apply(lambda x: x.shift(1).expanding(min_periods=1).mean()).reset_index(level=[0, 1], drop=True)\n",
    "Merged_nba_df['Expanding_Total_Points'].fillna(0,inplace=True)\n",
    "Merged_nba_df['Expanding_Opp_Total_Points'].fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be5af2",
   "metadata": {},
   "source": [
    "# Identifying Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d76ee370",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [ 'Season',\n",
    "              'G',\n",
    "              'Team_Code',\n",
    "              'Opp_Code',\n",
    "              'Home/Away',\n",
    "              'Tm_rolling',\n",
    "              'Opp.1_rolling',\n",
    "              'ORtg_rolling',\n",
    "              'DRtg_rolling',\n",
    "              'Pace_rolling',\n",
    "              'FTr_rolling',\n",
    "              '3PAr_rolling',\n",
    "              'TS%_rolling',\n",
    "              'TRB%_rolling',\n",
    "              'AST%_rolling',\n",
    "              'eFG%_rolling',\n",
    "              'TOV%_rolling',\n",
    "              'ORB%_rolling',\n",
    "              'FT/FGA_rolling',\n",
    "              'OppeFG%_rolling',\n",
    "              'OppTOV%_rolling',\n",
    "              'DRB%_rolling',\n",
    "              'OppFT/FGA_rolling',\n",
    "              'OppPoints_Allowed_rolling',\n",
    "              'OppPoints_Scored_rolling',\n",
    "              'OppDRtg_rolling',\n",
    "              'OppORtg_rolling',\n",
    "              'DefenisveeFG%_rolling',\n",
    "              'DefensiveTOV%_rolling',\n",
    "              'OppOppFT/FGA_rolling',\n",
    "              'OppOppeFG_rolling',\n",
    "              'OppOppTOV%_rolling',\n",
    "              'Opp2FT/FGA_rolling',\n",
    "              'eDIFF_rolling',\n",
    "    'OppeDIFF_rolling',\n",
    "                 'pDIFF_rolling',\n",
    "    'OpppDIFF_rolling',\n",
    "                 'TmeDIFFProduct',\n",
    "    'TmpDIFFProduct',\n",
    "    'TmOppAllowedProduct',\n",
    "              \"PaceTmProduct\",\n",
    "                  \"Rest_Days\",\n",
    "    \"Opp_Rest_Days\",\n",
    "    \"Experience\",\n",
    "              \"Total_Salary\",              \n",
    "              'Expanding_Total_Points','Expanding_Opp_Total_Points'\n",
    "              \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1ac68aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_nba_df[predictors] = Merged_nba_df[predictors].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fcc996c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Merged_nba_df['W/L'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f532e8",
   "metadata": {},
   "source": [
    "# For logistic regression I want to find all features that have significant p-values through backwards elimination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b41eeb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_logistic = Merged_nba_df[predictors]\n",
    "X_constant = add_constant(X_logistic)\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_constant)\n",
    "X_scaled = pd.DataFrame(X_scaled,columns=X_constant.columns, index= X_logistic.index)\n",
    "def backward_elimination(X_data,target, col_list):\n",
    "    \n",
    "    while len(col_list) > 0:\n",
    "        model = sm.Logit(target,X_data[col_list])\n",
    "        result = model.fit(disp=0)\n",
    "        largest_pvalue = round(result.pvalues, 3).nlargest(1)\n",
    "        if largest_pvalue[0] <(0.05):\n",
    "            return result, col_list\n",
    "            break\n",
    "        else:\n",
    "            col_list.remove(largest_pvalue.index)\n",
    "            \n",
    "result, optimal_columns = backward_elimination(X_constant,Merged_nba_df['W/L'], predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c8da8383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Season',\n",
       " 'Home/Away',\n",
       " 'Tm_rolling',\n",
       " 'DRtg_rolling',\n",
       " 'FTr_rolling',\n",
       " 'TS%_rolling',\n",
       " 'AST%_rolling',\n",
       " 'ORB%_rolling',\n",
       " 'OppeFG%_rolling',\n",
       " 'OppTOV%_rolling',\n",
       " 'DRB%_rolling',\n",
       " 'OppFT/FGA_rolling',\n",
       " 'OppPoints_Scored_rolling',\n",
       " 'OppORtg_rolling',\n",
       " 'DefenisveeFG%_rolling',\n",
       " 'DefensiveTOV%_rolling',\n",
       " 'OppOppeFG_rolling',\n",
       " 'OppOppTOV%_rolling',\n",
       " 'Opp2FT/FGA_rolling',\n",
       " 'pDIFF_rolling',\n",
       " 'OpppDIFF_rolling',\n",
       " 'TmpDIFFProduct',\n",
       " 'TmOppAllowedProduct',\n",
       " 'Rest_Days',\n",
       " 'Opp_Rest_Days',\n",
       " 'Experience',\n",
       " 'Total_Salary']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b35c39",
   "metadata": {},
   "source": [
    "# Finally we can predict a win or loss using our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f41ac35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 67.836%\n",
      "Test Accuracy: 64.784%\n",
      "Train Accuracy: 68.102%\n",
      "Validation ROC AUC: 0.743\n",
      "Test ROC AUC: 0.706\n",
      "Train ROC AUC: 0.747\n",
      "\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68      2011\n",
      "           1       0.68      0.67      0.68      2009\n",
      "\n",
      "    accuracy                           0.68      4020\n",
      "   macro avg       0.68      0.68      0.68      4020\n",
      "weighted avg       0.68      0.68      0.68      4020\n",
      "\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.63      0.64     13516\n",
      "           1       0.64      0.67      0.65     13500\n",
      "\n",
      "    accuracy                           0.65     27016\n",
      "   macro avg       0.65      0.65      0.65     27016\n",
      "weighted avg       0.65      0.65      0.65     27016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training, validation, and testing sets based on the 'Season' column\n",
    "logits_train = Merged_nba_df[Merged_nba_df['Season'] <= 2008]\n",
    "logits_val = Merged_nba_df[(Merged_nba_df['Season'] > 2008) & (Merged_nba_df['Season'] <= 2010)]\n",
    "logits_test = Merged_nba_df[Merged_nba_df['Season'] > 2010]\n",
    "\n",
    "# Define predictors and target variable\n",
    "X_logits_train = logits_train[optimal_columns]\n",
    "logits_y_train = logits_train['W/L']\n",
    "X_logits_val = logits_val[optimal_columns]\n",
    "logits_y_val = logits_val['W/L']\n",
    "X_logits_test = logits_test[optimal_columns]\n",
    "logits_y_test = logits_test['W/L']\n",
    "\n",
    "# Standardize the features\n",
    "scaler_logits = StandardScaler()\n",
    "X_logits_train_scaled = scaler_logits.fit_transform(X_logits_train)\n",
    "X_logits_val_scaled = scaler_logits.transform(X_logits_val)\n",
    "X_logits_test_scaled = scaler_logits.transform(X_logits_test)\n",
    "\n",
    "# pca = PCA(n_components=3).fit(X_logits_train_scaled)\n",
    "# X_logits_train_PCA = pca.transform(X_logits_train_scaled)\n",
    "# X_logits_test_PCA = pca.transform(X_logits_test_scaled)\n",
    "# X_logits_val_PCA = pca.transform(X_logits_val_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "lclf = LogisticRegression(random_state=0, max_iter=1000,solver='liblinear', penalty=\"l2\", C=5.0)\n",
    "lclf.fit(X_logits_train_scaled, logits_y_train)\n",
    "\n",
    "# Predict and evaluate on the validation set\n",
    "y_val_pred = lclf.predict(X_logits_val_scaled)\n",
    "y_val_prob = lclf.predict_proba(X_logits_val_scaled)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_test_pred = lclf.predict(X_logits_test_scaled)\n",
    "y_test_prob = lclf.predict_proba(X_logits_test_scaled)\n",
    "\n",
    "# Predict and evaluate on the training set\n",
    "train_y_pred = lclf.predict(X_logits_train_scaled)\n",
    "train_y_prob = lclf.predict_proba(X_logits_train_scaled)\n",
    "\n",
    "# Calculate accuracies\n",
    "val_accuracy = lclf.score(X_logits_val_scaled, logits_y_val)\n",
    "print(\"Validation Accuracy: %.3f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "test_accuracy = lclf.score(X_logits_test_scaled, logits_y_test)\n",
    "print(\"Test Accuracy: %.3f%%\" % (test_accuracy * 100.0))\n",
    "\n",
    "train_accuracy = lclf.score(X_logits_train_scaled, logits_y_train)\n",
    "print(\"Train Accuracy: %.3f%%\" % (train_accuracy * 100.0))\n",
    "\n",
    "# Calculate ROC AUC scores\n",
    "roc_auc_val = roc_auc_score(logits_y_val, y_val_prob[:, 1])\n",
    "print(\"Validation ROC AUC: %.3f\" % roc_auc_val)\n",
    "\n",
    "roc_auc_test = roc_auc_score(logits_y_test, y_test_prob[:, 1])\n",
    "print(\"Test ROC AUC: %.3f\" % roc_auc_test)\n",
    "\n",
    "roc_auc_train = roc_auc_score(logits_y_train, train_y_prob[:, 1])\n",
    "print(\"Train ROC AUC: %.3f\" % roc_auc_train)\n",
    "\n",
    "# Print classification report for validation and test sets\n",
    "print(\"\\nValidation Classification Report:\\n\", classification_report(logits_y_val, y_val_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(logits_y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c58936",
   "metadata": {},
   "source": [
    "# Model ranges from 67% accuracy to 64%. Also has a good ROC_AUC.\n",
    "# Lets try and identify a strategy if I only chose games to bet on when my model has 80% confidence or higher it has ~ 80% accuracy with calling games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "09220f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Confidence Correct Predictions: 2242\n",
      "High Confidence Incorrect Predictions: 572\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(X_logits_test, columns=X_logits_test.columns)\n",
    "result_df['G'] = logits_test['G']\n",
    "result_df['Tm'] = logits_test['Tm']\n",
    "result_df['Opp.1'] = logits_test['Opp.1']\n",
    "result_df['Team'] = logits_test['Team']\n",
    "result_df['Opp'] = logits_test['Opp']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result_df['actual'] = logits_y_test\n",
    "result_df['predicted'] = y_test_pred\n",
    "result_df['predicted_prob'] = y_test_prob[:,1]  # Only one column for probability of the positive class\n",
    "result_df['confidence'] = result_df['predicted_prob']\n",
    "result_df['high_confidence'] = result_df['confidence'] >= .80\n",
    "\n",
    "# Filter high confidence predictions\n",
    "high_confidence_df = result_df[result_df['high_confidence']]\n",
    "high_confidence_df['predicted'] = 1\n",
    "# Count correct and incorrect high confidence predictions\n",
    "high_confidence_correct = high_confidence_df[high_confidence_df['actual'] == high_confidence_df['predicted']].shape[0]\n",
    "high_confidence_incorrect = high_confidence_df[high_confidence_df['actual'] != high_confidence_df['predicted']].shape[0]\n",
    "\n",
    "print(f\"High Confidence Correct Predictions: {high_confidence_correct}\")\n",
    "print(f\"High Confidence Incorrect Predictions: {high_confidence_incorrect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725d65f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
